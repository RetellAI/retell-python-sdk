"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
import requests as requests_http
from ...models.components import agent as components_agent
from dataclasses_json import Undefined, dataclass_json
from typing import Optional, Union
from retellclient import utils


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class UpdateAgentRequestBody:
    voice_id: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('voice_id'), 'exclude': lambda f: f is None }})
    r"""Unique voice id used for the agent. Find list of available voices in documentation."""
    agent_name: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('agent_name'), 'exclude': lambda f: f is None }})
    r"""The name of the agent. Only used for your own reference."""
    llm_setting: Optional[Union[components_agent.RetellLlmSetting, components_agent.CustomLlmSetting]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('llm_setting'), 'exclude': lambda f: f is None }})
    r"""Determines how to generate the response in the call. Currently supports using our in-house LLM response system or your own custom response generation system."""
    interaction_setting: Optional[components_agent.InteractionSettingRequest] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('interaction_setting'), 'exclude': lambda f: f is None }})
    r"""Setting combination that controls interaction flow, like begin and end logic."""
    functions: Optional[List[Function]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('functions'), 'exclude': lambda f: f is None }})
    r"""Functions are the actions that the agent can perform, like booking appointments, retriving information, etc. By setting this field, either OpenAI's function calling feature or your own custom LLM's logic would determine when the function shall get called, and our server would make the call."""

@dataclasses.dataclass
class UpdateAgentRequest:
    agent_id: str = dataclasses.field(metadata={'path_param': { 'field_name': 'agent_id', 'style': 'simple', 'explode': False }})
    r"""Unique id of the agent to be updated."""
    request_body: UpdateAgentRequestBody = dataclasses.field(metadata={'request': { 'media_type': 'application/json' }})


@dataclasses.dataclass
class UpdateAgentResponse:
    content_type: str = dataclasses.field()
    r"""HTTP response content type for this operation"""
    raw_response: requests_http.Response = dataclasses.field()
    r"""Raw HTTP response; suitable for custom response parsing"""
    status_code: int = dataclasses.field()
    r"""HTTP response status code for this operation"""
    agent: Optional[components_agent.Agent] = dataclasses.field(default=None)
    r"""Successfully updated an agent."""
    

